# vLLM Micro-Queries with Prompt Caching

> **Note**: This entire project (code, dataset, documentation) was generated by an AI agent (opencode/glm-4.7-free).

Benchmark micro-query evaluation with prompt caching/KV reuse using local open-weight models.

## Background & Goal

This project benchmarks the performance and correctness of an implementation that uses **prompt caching / KV cache reuse** for repeated micro-queries on the same context.

The goal is to demonstrate how prompt caching can significantly reduce computation time when:
- Running many micro-queries (yes/no questions) on the same text context
- The context is shared across all queries
- Each query needs the full context to provide accurate answers

## How It Works

1. **Context Processing**: The text context is processed once and its Key-Value (KV) cache states are stored
2. **Query Evaluation**: Each micro-query is evaluated by appending the question to the cached context prefix
3. **Cache Reuse**: The shared context prefix is not re-computed, saving significant time
4. **Independent Evaluation**: Each micro-query is evaluated independently, returning "YES" or "NO"

This approach is particularly beneficial when running many queries on the same context, as it avoids redundant computation of the context embeddings and attention states.

## Dataset

The synthetic test dataset focuses on **reading comprehension** with narrative texts that test:
- Understanding of character actions and motivations
- Sequencing of events
- Recall of specific details
- Inference from context
- Following multi-step narratives

The dataset includes:
- **52 contexts**: 11 narrative reading comprehension stories (including 1 large context with 309 questions)
- **993 micro-queries**: All yes/no questions with expected answers for validation

Reading comprehension narratives (11 contexts):
- Job interview preparation (ctx011)
- Writer's block and cat companion (ctx012)
- Family reunion gathering (ctx013)
- Shopping trip to department store (ctx014)
- Bookstore café meeting (ctx015)
- Morning exercise routine (ctx016)
- Dinner party with friends (ctx017)
- Hotel stay at coastal location (ctx018)
- Child's piano lesson (ctx019)
- Power outage during storm (ctx020)
- Library visit (ctx041)
- Cooking a meal (ctx042)
- Hiking trip (ctx043)
- Attending a concert (ctx044)
- Working from home (ctx045)
- Museum visit (ctx046)
- Gardening project (ctx047)
- Learning to swim (ctx048)
- Garage sale (ctx049)
- Train journey (ctx050)
- **Wilson family vacation (ctx051)**: Large context with 309 questions testing detailed comprehension of a week-long camping trip

Each context has 10-309 associated micro-queries that test detailed comprehension of the story. Context ctx051 serves as a stress test with hundreds of questions on a single narrative to evaluate how KV cache performance scales with many queries on a single context.

## Benchmark Results (Qwen2.5-0.5B-Instruct)

| Metric | Value |
|--------|-------|
| Overall Accuracy | 80.06% (795/993) |
| Average Query Time | ~0.02s |
| Total Evaluation Time | ~30-35s for 993 queries |
| Context Processing Time | ~0.05-0.07s per context |

Best performing contexts (100% accuracy):
- ctx006: Quantum mechanics
- ctx009: The Renaissance
- ctx027: Ancient Egypt
- ctx030: Artificial neural networks
- ctx034: Photosynthesis vs cellular respiration
- ctx035: Stock market
- ctx039: Binary code
- ctx040: Cardiovascular system

High performing reading comprehension contexts (>85% accuracy):
- ctx051: Wilson family vacation (85.11%, 309 questions) - **Large context stress test**
- ctx048: Learning to swim (87.50%)
- ctx049: Garage sale (85.00%)
- ctx016: Morning exercise (86.67%)
- ctx020: Power outage (86.67%)

More challenging contexts (reading comprehension with narrative details):
- ctx045: Working from home (27.27%)
- ctx013: Family reunion (46.67%)
- ctx015: Bookstore meeting (53.33%)
- ctx017: Dinner party (46.67%)
- ctx050: Train journey (59.09%)

### Performance Notes
- Scientific and technical contexts generally achieve higher accuracy (80-100%)
- Reading comprehension narratives with multiple character interactions are more challenging (27-87%)
- **KV cache scaling**: ctx051 (309 queries) maintains consistent query times (~0.020s) with smaller contexts, demonstrating effective KV cache reuse
- **Context length impact**: Despite being 3-4× longer than average contexts, ctx051 (2280 chars) shows no performance degradation
- **Historical contexts perform well (80-100%)**


### Context ctx051: Wilson Family Vacation (Stress Test)

Context ctx051 contains a detailed narrative about the Wilson family's week-long camping trip to Lake Tahoe, with **309 micro-queries** testing comprehensive reading comprehension. This serves as a stress test to evaluate KV cache performance when processing hundreds of queries on a single context.

**Context details:**
- Story: Wilson family (Mr. Wilson, Mrs. Wilson, Emma (10), Lucas (8)) camping vacation
- Timeline: Saturday departure through Friday return
- Activities: Swimming, fishing, hiking, boating, campfires, souvenir shopping
- Questions: 309 total, covering family details, timeline, activities, and negative inferences

**Benchmark Results (Qwen2.5-0.5B-Instruct):**

| Metric | Value |
|--------|-------|
| **Accuracy** | 85.11% (263/309 correct) |
| **Context Processing Time** | 0.057s |
| **Total Evaluation Time** | 6.332s for 309 queries |
| **Average Query Time** | 0.020s |
| **Min Query Time** | 0.017s |
| **Max Query Time** | 0.024s |
| **Context Length** | 2280 characters |

**Key observations:**
- **KV cache efficiency**: Average query time (0.020s) remains consistent with smaller contexts, demonstrating that KV cache scales effectively to 309 queries
- **Performance stability**: Query times have low variance (min: 0.017s, max: 0.024s), showing stable cache performance across all queries
- **Accuracy on complex narrative**: 85.11% accuracy is strong for a comprehensive narrative with 309 questions, significantly outperforming shorter narratives (ctx045: 27.27%, ctx050: 59.09%)
- **One-time processing cost**: Context processing time (0.057s) represents ~0.9% of total time, with 99.1% of time spent on queries using cached KV states
- **Amortized efficiency**: With 309 queries, the per-query context processing cost amortizes to 0.00018s per query (0.057s / 309)

**Comparison with smaller contexts:**
- Average query time for ctx051 (0.020s) matches other contexts, confirming KV cache maintains efficiency at scale
- Accuracy (85.11%) ranks among the highest for reading comprehension narratives, comparable to ctx016 (86.67%), ctx020 (86.67%), ctx048 (87.50%)
- Context length (2280 chars) is ~3-4× longer than typical contexts, yet query performance remains unchanged

## Requirements

```bash
pip install vllm
```

## Hardware Used for Benchmarking

- **GPU**: NVIDIA RTX 4090 (24GB GDDR6X)
- **CPU**: AMD Ryzen 7 7800X3D (8-core/16-thread)
- **RAM**: 62 GB DDR5
- **OS**: Linux fedora 6.17.9-200.fc42.x86_64

## Usage

Run with the default 3B parameter model (Qwen2.5-3B-Instruct):

```bash
python microqueries.py
```

Run with a specific local model:

```bash
python microqueries.py --model microsoft/Phi-3-mini-4k-instruct --gpu-memory-utilization 0.5
```

Run with timeout (recommended for benchmarks):

```bash
timeout 300 python microqueries.py --model Qwen/Qwen2.5-0.5B-Instruct --gpu-memory-utilization 0.5
```

Run with verbose output:

```bash
python microqueries.py --verbose
```

Disable prompt caching for comparison:

```bash
python microqueries.py --no-cache
```

Adjust GPU memory utilization (default 0.8):

```bash
python microqueries.py --gpu-memory-utilization 0.5
```

## Recommended Local Models (Under 10B Parameters)

| Model | Size | Quality | Speed |
|-------|------|---------|-------|
| `Qwen/Qwen2.5-3B-Instruct` | 3B | Excellent | Fast |
| `microsoft/Phi-3-mini-4k-instruct` | 3.8B | Excellent | Fast |
| `meta-llama/Llama-3.2-3B-Instruct` | 3B | Very Good | Fast |
| `Qwen/Qwen2.5-0.5B-Instruct` | 0.5B | Good | Very Fast |
| `meta-llama/Llama-3.2-1B-Instruct` | 1B | Good | Very Fast |

## Implementation Details

The implementation uses vLLM's built-in prefix caching (`enable_prefix_caching=True`) which automatically:
- Detects identical prompt prefixes across requests
- Stores and reuses KV cache states for those prefixes
- Avoids redundant computation of shared context embeddings

**Current limitations:**
- Questions are evaluated independently without considering dependencies
- No support for conditional question execution (e.g., skipping follow-up questions based on parent answers)
- No confidence estimation or adaptive questioning strategies

The benchmark tracks:
- Context processing time (first pass)
- Per-query evaluation time (with cache hits)
- Cache hit/miss status
- Overall accuracy against expected answers

**Future enhancement**: Question trees with adaptive querying could dramatically reduce total queries by 30-70% for complex narratives while maintaining accuracy.

## Files

- `microqueries.py` - Main implementation with vLLM and KV caching
- `dataset.tsv` - Context texts (52 entries)
- `questions.tsv` - Micro-queries with expected answers (993 entries)
- `validate.py` - Validate dataset format
- `prompt.md` - Original task description
- `AGENTS.md` - AI agent usage and development workflow
- `HARDWARE.md` - Hardware specifications used for benchmarking (gitignored)


## Future Exploration

### Question Trees and Adaptive Querying

A critical limitation of the current micro-query system is that questions are evaluated independently, without considering dependencies between them. This can lead to inefficient or contradictory queries. For example:

```
Q1: "Did the protagonist remove any clothing?" → NO (high confidence)
Q2: "Did the protagonist remove their jacket?" → NO (redundant if Q1 answered NO)
```

In this scenario, Q2 becomes unnecessary or confusing if Q1 was answered "NO" with high confidence. **Question trees** represent a natural extension where:

1. **Dependency Tracking**: Questions are organized as a DAG (Directed Acyclic Graph) where child questions depend on parent answers
2. **Conditional Execution**: Questions are only evaluated if their parent questions satisfy specific conditions
3. **Optimization**: The query order can be optimized to minimize total evaluation time and avoid redundant queries
4. **Confidence-Based Pruning**: High-confidence answers to parent questions can skip entire subtrees of follow-up questions

**Potential approaches:**

- **Static question trees**: Pre-defined dependency structures encoded in the dataset format
- **Dynamic dependency detection**: Using a separate model to identify question dependencies on-the-fly
- **Confidence thresholds**: Skip follow-up questions when parent answer confidence exceeds threshold (e.g., >0.95)
- **Adaptive questioning**: Reorder questions dynamically based on model confidence and information gain
- **Multi-model switching**: Use smaller/faster models (e.g., 0.5B params) for initial broad questions to quickly eliminate irrelevant branches, then switch to more accurate models (e.g., 3B params) for follow-up details in relevant branches. This leverages the insight that not all questions require the same model quality - broad screening questions can tolerate lower accuracy if they lead to pruning irrelevant subtrees.

**Example multi-model strategy:**
```
Layer 1 (fast 0.5B model): Broad screening questions
└─ "Did the character wear any clothing?" → NO → SKIP clothing subtree
    └─ "Did the character remove any clothing?" → YES
        └─ Layer 2 (accurate 3B model): Detailed questions
            └─ "What type of clothing was removed?"
            └─ "When did they remove it?"
```

**Example question tree structure:**
```
Did the protagonist enter a building?
├─ YES → Did the protagonist enter a shop?
│   └─ YES → Did the protagonist buy anything?
│       ├─ YES → What did they buy? (open-ended)
│       └─ NO → Did they try on clothes?
└─ NO → Was the protagonist outdoors? → [continue outdoor queries]
```

**Research questions:**
- How can we efficiently encode and parse question dependency structures?
- What confidence thresholds optimally balance accuracy vs. computational efficiency?
- Can reinforcement learning optimize adaptive questioning strategies?
- How do question trees interact with KV cache efficiency (fewer queries = less cache reuse)?

Implementing question trees would require:
- Extended dataset format supporting parent-child relationships
- Dynamic query scheduling logic in the evaluation pipeline
- Confidence estimation mechanisms from model outputs
- Dependency graph traversal and pruning algorithms

If you're interested in implementing adaptive questioning with question trees, this would dramatically improve efficiency for complex narratives and reduce total evaluation time while maintaining or improving accuracy.

### Continuous Confidence Scoring

A natural extension of this binary micro-query system is a nuanced scoring layer that maps coarse discrete outputs (e.g., -1 / 1) onto a continuous spectrum reflecting uncertainty, context ambiguity, or probabilistic reasoning. For example, a micro-query that initially outputs `-1` could be recalibrated to `-0.35` or `-0.8` depending on subtle contextual cues or societal considerations embedded in the input text. Approaches for this could include statistical calibration methods (such as Platt scaling or isotonic regression), secondary model passes to reinterpret discrete labels, or hybrid heuristics that weigh external knowledge and contextual factors. The goal is to preserve interpretability while introducing graded confidence, enabling more fine-grained analysis across large sets of queries.

Implementing this type of system would benefit from candidates or contributors with experience in machine learning calibration, probabilistic modeling, and NLP system design, with strong practical skills in Python and transformer-based architectures. Key competencies include designing and evaluating scoring functions, handling large-scale micro-query inference workflows, and translating discrete model outputs into continuous, interpretable metrics. Familiarity with confidence estimation, multi-step reasoning, and robust evaluation frameworks is highly valuable for extending the system beyond simple binary responses. If you're that kind of person or AI agent, please fork and implement; I'll acknowledge and praise your work. Heck, you could even claim all of this work as your own and continue from there; I (Joris, not Joris' software development agent) don't mind :)