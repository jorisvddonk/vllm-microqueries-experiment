# vLLM Micro-Queries with Prompt Caching

> **Note**: This entire project (code, dataset, documentation) was generated by an AI agent (opencode/glm-4.7-free).

Benchmark micro-query evaluation with prompt caching/KV reuse using local open-weight models.

## Background & Goal

This project benchmarks the performance and correctness of an implementation that uses **prompt caching / KV cache reuse** for repeated micro-queries on the same context.

The goal is to demonstrate how prompt caching can significantly reduce computation time when:
- Running many micro-queries (yes/no questions) on the same text context
- The context is shared across all queries
- Each query needs the full context to provide accurate answers

## How It Works

1. **Context Processing**: The text context is processed once and its Key-Value (KV) cache states are stored
2. **Query Evaluation**: Each micro-query is evaluated by appending the question to the cached context prefix
3. **Cache Reuse**: The shared context prefix is not re-computed, saving significant time
4. **Independent Evaluation**: Each micro-query is evaluated independently, returning "YES" or "NO"

This approach is particularly beneficial when running many queries on the same context, as it avoids redundant computation of the context embeddings and attention states.

## Dataset

The synthetic test dataset includes:
- **41 contexts**: A mix of science/history/technical topics and prose/reading comprehension narratives
- **487 micro-queries**: All yes/no questions with expected answers for validation

Topics covered:
- Science: Solar system, photosynthesis, DNA, quantum mechanics, heart, climate change, plate tectonics, water cycle, immune system, viruses, biomes, cellular respiration
- History: Industrial Revolution, American Civil War, Renaissance, French Revolution, World War I, Ancient Egypt
- Technology: Machine learning, binary code, electricity, stock market
- Narratives: 10 reading comprehension stories with daily life scenarios

Each context has 8-21 associated micro-queries that test factual understanding of text.

## Benchmark Results (Qwen2.5-0.5B-Instruct)

| Metric | Value |
|--------|-------|
| Overall Accuracy | 81.31% (396/487) |
| Average Query Time | ~0.02s |
| Total Evaluation Time | ~8-10s for 487 queries |
| Context Processing Time | ~0.05-0.07s per context |

Best performing contexts (100% accuracy):
- ctx006: Quantum mechanics
- ctx009: The Renaissance
- ctx027: Ancient Egypt
- ctx030: Artificial neural networks
- ctx034: Photosynthesis vs cellular respiration
- ctx035: Stock market
- ctx039: Binary code
- ctx040: Cardiovascular system

More challenging contexts (reading comprehension with narrative details):
- ctx013: Family reunion (46.67%)
- ctx015: Bookstore meeting (53.33%)
- ctx017: Dinner party (46.67%)

### Performance Notes
- Scientific and technical contexts generally achieve higher accuracy (80-100%)
- Reading comprehension narratives with multiple character interactions are more challenging (46-86%)
- Historical contexts perform well (80-100%)

## Requirements

```bash
pip install vllm
```

## Hardware Used for Benchmarking

- **GPU**: NVIDIA RTX 4090 (24GB GDDR6X)
- **CPU**: AMD Ryzen 7 7800X3D (8-core/16-thread)
- **RAM**: 62 GB DDR5
- **OS**: Linux fedora 6.17.9-200.fc42.x86_64

## Usage

Run with the default 3B parameter model (Qwen2.5-3B-Instruct):

```bash
python microqueries.py
```

Run with a specific local model:

```bash
python microqueries.py --model microsoft/Phi-3-mini-4k-instruct --gpu-memory-utilization 0.5
```

Run with timeout (recommended for benchmarks):

```bash
timeout 300 python microqueries.py --model Qwen/Qwen2.5-0.5B-Instruct --gpu-memory-utilization 0.5
```

Run with verbose output:

```bash
python microqueries.py --verbose
```

Disable prompt caching for comparison:

```bash
python microqueries.py --no-cache
```

Adjust GPU memory utilization (default 0.8):

```bash
python microqueries.py --gpu-memory-utilization 0.5
```

## Recommended Local Models (Under 10B Parameters)

| Model | Size | Quality | Speed |
|-------|------|---------|-------|
| `Qwen/Qwen2.5-3B-Instruct` | 3B | Excellent | Fast |
| `microsoft/Phi-3-mini-4k-instruct` | 3.8B | Excellent | Fast |
| `meta-llama/Llama-3.2-3B-Instruct` | 3B | Very Good | Fast |
| `Qwen/Qwen2.5-0.5B-Instruct` | 0.5B | Good | Very Fast |
| `meta-llama/Llama-3.2-1B-Instruct` | 1B | Good | Very Fast |

## Implementation Details

The implementation uses vLLM's built-in prefix caching (`enable_prefix_caching=True`) which automatically:
- Detects identical prompt prefixes across requests
- Stores and reuses KV cache states for those prefixes
- Avoids redundant computation of shared context embeddings

The benchmark tracks:
- Context processing time (first pass)
- Per-query evaluation time (with cache hits)
- Cache hit/miss status
- Overall accuracy against expected answers

## Files

- `microqueries.py` - Main implementation with vLLM and KV caching
- `dataset.tsv` - Context texts (41 entries)
- `questions.tsv` - Micro-queries with expected answers (487 entries)
- `validate.py` - Validate dataset format
- `prompt.md` - Original task description
- `AGENTS.md` - AI agent usage and development workflow
- `HARDWARE.md` - Hardware specifications used for benchmarking (gitignored)
