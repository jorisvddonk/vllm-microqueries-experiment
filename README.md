# vLLM Micro-Queries with Prompt Caching

> **Note**: This entire project (code, dataset, documentation) was generated by an AI agent (opencode/glm-4.7-free).

Benchmark micro-query evaluation with prompt caching/KV reuse using local open-weight models.

## Background & Goal

This project benchmarks the performance and correctness of an implementation that uses **prompt caching / KV cache reuse** for repeated micro-queries on the same context.

The goal is to demonstrate how prompt caching can significantly reduce computation time when:
- Running many micro-queries (yes/no questions) on the same text context
- The context is shared across all queries
- Each query needs the full context to provide accurate answers

## How It Works

1. **Context Processing**: The text context is processed once and its Key-Value (KV) cache states are stored
2. **Query Evaluation**: Each micro-query is evaluated by appending the question to the cached context prefix
3. **Cache Reuse**: The shared context prefix is not re-computed, saving significant time
4. **Independent Evaluation**: Each micro-query is evaluated independently, returning "YES" or "NO"

This approach is particularly beneficial when running many queries on the same context, as it avoids redundant computation of the context embeddings and attention states.

## Dataset

The synthetic test dataset includes:
- **20 contexts**: A mix of science/history/technical topics and prose/reading comprehension narratives
- **280 micro-queries**: All yes/no questions with expected answers for validation

Each context has 10-21 associated micro-queries that test factual understanding of the text.

## Benchmark Results (Qwen2.5-0.5B-Instruct)

| Metric | Value |
|--------|-------|
| Overall Accuracy | 74.64% (209/280) |
| Average Query Time | ~0.02s |
| Total Evaluation Time | ~30-40s for 280 queries |
| Context Processing Time | ~0.05-0.07s per context |

Best performing contexts (100% accuracy):
- ctx006: Quantum mechanics
- ctx009: The Renaissance

More challenging contexts (reading comprehension with narrative details):
- ctx013: Family reunion (47%)
- ctx015: Bookstore meeting (53%)
- ctx017: Dinner party (47%)

## Requirements

```bash
pip install vllm
```

## Usage

Run with the default 3B parameter model (Qwen2.5-3B-Instruct):

```bash
python microqueries.py
```

Run with a specific local model:

```bash
python microqueries.py --model microsoft/Phi-3-mini-4k-instruct
```

Run with verbose output:

```bash
python microqueries.py --verbose
```

Disable prompt caching for comparison:

```bash
python microqueries.py --no-cache
```

Adjust GPU memory utilization (default 0.8):

```bash
python microqueries.py --gpu-memory-utilization 0.5
```

## Recommended Local Models (Under 10B Parameters)

| Model | Size | Quality | Speed |
|-------|------|---------|-------|
| `Qwen/Qwen2.5-3B-Instruct` | 3B | Excellent | Fast |
| `microsoft/Phi-3-mini-4k-instruct` | 3.8B | Excellent | Fast |
| `meta-llama/Llama-3.2-3B-Instruct` | 3B | Very Good | Fast |
| `Qwen/Qwen2.5-0.5B-Instruct` | 0.5B | Good | Very Fast |
| `meta-llama/Llama-3.2-1B-Instruct` | 1B | Good | Very Fast |

## Implementation Details

The implementation uses vLLM's built-in prefix caching (`enable_prefix_caching=True`) which automatically:
- Detects identical prompt prefixes across requests
- Stores and reuses KV cache states for those prefixes
- Avoids redundant computation of shared context embeddings

The benchmark tracks:
- Context processing time (first pass)
- Per-query evaluation time (with cache hits)
- Cache hit/miss status
- Overall accuracy against expected answers

## Files

- `microqueries.py` - Main implementation with vLLM and KV caching
- `dataset.tsv` - Context texts (20 entries)
- `questions.tsv` - Micro-queries with expected answers (280 entries)
- `validate.py` - Validate dataset format
- `prompt.md` - Original task description
